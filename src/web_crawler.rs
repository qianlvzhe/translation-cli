//! Webçˆ¬å–æ¨¡å— - é›†æˆMonolithè¿›è¡Œç½‘é¡µå†…å®¹æŠ“å–å’Œå¤„ç†
//! 
//! æ­¤æ¨¡å—è´Ÿè´£ï¼š
//! - ä½¿ç”¨Monolithåº“æŠ“å–å®Œæ•´çš„ç½‘é¡µå†…å®¹
//! - å°†ç½‘é¡µè½¬æ¢ä¸ºç‹¬ç«‹çš„HTMLæ–‡ä»¶ï¼ˆåŒ…å«CSSã€JSã€å›¾ç‰‡ç­‰èµ„æºï¼‰
//! - ä¸ºåç»­çš„ç¿»è¯‘å¤„ç†å‡†å¤‡æ ‡å‡†åŒ–çš„HTMLå†…å®¹

// æ ‡å‡†åº“å¯¼å…¥
use std::path::{Path, PathBuf};

// ç¬¬ä¸‰æ–¹crateå¯¼å…¥
use anyhow::{Context, Result};
use tracing::{debug, info, warn};

/// Webçˆ¬è™«é…ç½®ç»“æ„ä½“
#[derive(Debug, Clone)]
pub struct WebCrawlerConfig {
    /// ç›®æ ‡URL
    pub url: String,
    /// è¾“å‡ºæ–‡ä»¶è·¯å¾„
    pub output_path: PathBuf,
    /// æ˜¯å¦åŒ…å«CSSæ ·å¼
    pub include_css: bool,
    /// æ˜¯å¦åŒ…å«JavaScript
    pub include_js: bool,
    /// æ˜¯å¦åŒ…å«å›¾ç‰‡èµ„æº
    pub include_images: bool,
    /// ç”¨æˆ·ä»£ç†å­—ç¬¦ä¸²
    pub user_agent: String,
    /// è¿æ¥è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    pub timeout: u64,
}

impl Default for WebCrawlerConfig {
    fn default() -> Self {
        Self {
            url: String::new(),
            output_path: PathBuf::new(),
            include_css: true,
            include_js: false, // é»˜è®¤ä¸åŒ…å«JSï¼Œé¿å…æ½œåœ¨çš„å®‰å…¨é—®é¢˜
            include_images: true,
            user_agent: "translation-cli/0.1.0 (Monolith Web Crawler)".to_string(),
            timeout: 30,
        }
    }
}

/// Webçˆ¬è™«ä¸»è¦ç»“æ„ä½“
/// 
/// é›†æˆMonolithåº“å®ç°ç½‘é¡µå†…å®¹æŠ“å–å’Œå¤„ç†åŠŸèƒ½ã€‚
/// æ”¯æŒè‡ªå®šä¹‰èµ„æºåŒ…å«ç­–ç•¥ã€é‡è¯•æœºåˆ¶å’Œå®‰å…¨é…ç½®ã€‚
/// 
/// # Features
/// 
/// - å®Œæ•´é¡µé¢ä¿å­˜ï¼šå°†CSSã€å›¾ç‰‡ç­‰èµ„æºå†…åµŒä¸ºdata URL
/// - å¼‚æ­¥å°è£…ï¼šä½¿ç”¨tokio::spawn_blockingå°è£…blocking API
/// - é‡è¯•æœºåˆ¶ï¼šæœ€å¤š3æ¬¡é‡è¯•ï¼ŒæŒ‡æ•°é€€é¿å»¶è¿Ÿ
/// - èµ„æºé…ç½®ï¼šå¯é€‰æ‹©æ€§åŒ…å«CSSã€JSã€å›¾ç‰‡ç­‰èµ„æº
pub struct WebCrawler {
    config: WebCrawlerConfig,
}

impl WebCrawler {
    /// ä½¿ç”¨Monolithåº“è¿›è¡Œå®é™…çš„ç½‘é¡µçˆ¬å–
    async fn crawl_website(&self) -> Result<String> {
        let config = &self.config;
        
        // åˆ›å»ºmonolithé€‰é¡¹
        let mut options = monolith::core::Options {
            no_css: !config.include_css,
            no_js: !config.include_js,
            no_images: !config.include_images,
            user_agent: Some(config.user_agent.clone()),
            timeout: config.timeout,
            ignore_errors: false,
            silent: true, // æˆ‘ä»¬ä½¿ç”¨è‡ªå·±çš„æ—¥å¿—ç³»ç»Ÿ
            ..Default::default()
        };

        debug!("Monolithé€‰é¡¹: no_css={}, no_js={}, no_images={}, timeout={}s", 
            options.no_css, options.no_js, options.no_images, options.timeout);

        let target_url = config.url.clone();

        // åœ¨blockingçº¿ç¨‹ä¸­æ‰§è¡Œmonolithæ“ä½œ
        let result = tokio::task::spawn_blocking(move || {
            use monolith::core::create_monolithic_document;
            use monolith::cache::Cache;
            
            // åˆ›å»ºç¼“å­˜ï¼Œè®¾ç½®æœ€å°æ–‡ä»¶å¤§å°ä¸º0ï¼Œä¸ä½¿ç”¨ç£ç›˜ç¼“å­˜æ–‡ä»¶
            let mut cache: Option<Cache> = Some(Cache::new(0, None));
            
            create_monolithic_document(target_url, &mut options, &mut cache)
        })
        .await
        .with_context(|| "Monolithä»»åŠ¡æ‰§è¡Œå¤±è´¥")?;

        match result {
            Ok((html_bytes, title)) => {
                let html_content = String::from_utf8(html_bytes)
                    .with_context(|| "è½¬æ¢HTMLå­—èŠ‚ä¸ºå­—ç¬¦ä¸²å¤±è´¥")?;
                
                if let Some(page_title) = title {
                    info!("ğŸ“„ ç½‘é¡µæ ‡é¢˜: {}", page_title);
                }
                info!("âœ… ç½‘é¡µå†…å®¹çˆ¬å–å®Œæˆï¼Œå¤§å°: {} å­—èŠ‚", html_content.len());
                
                Ok(html_content)
            }
            Err(e) => {
                anyhow::bail!("Monolithçˆ¬å–å¤±è´¥: {}", e);
            }
        }
    }

    /// å¸¦é‡è¯•æœºåˆ¶çš„ç½‘é¡µçˆ¬å–
    async fn crawl_website_with_retry(&self) -> Result<String> {
        const MAX_RETRIES: u32 = 3;
        let mut last_error: Option<anyhow::Error> = None;

        for attempt in 1..=MAX_RETRIES {
            info!("ğŸ”„ å°è¯•çˆ¬å–ç½‘é¡µ (ç¬¬ {} æ¬¡)", attempt);
            
            match self.crawl_website().await {
                Ok(content) => {
                    if attempt > 1 {
                        info!("âœ… é‡è¯•æˆåŠŸï¼");
                    }
                    return Ok(content);
                }
                Err(e) => {
                    warn!("âŒ çˆ¬å–å¤±è´¥ (å°è¯• {}/{}): {}", attempt, MAX_RETRIES, e);
                    last_error = Some(e);
                    
                    if attempt < MAX_RETRIES {
                        let delay = std::time::Duration::from_secs(attempt as u64 * 2);
                        info!("â³ ç­‰å¾… {:?} åé‡è¯•...", delay);
                        tokio::time::sleep(delay).await;
                    }
                }
            }
        }

        Err(last_error.unwrap_or_else(|| anyhow::anyhow!("æ‰€æœ‰é‡è¯•å°è¯•å‡å¤±è´¥")))
    }
    /// åˆ›å»ºæ–°çš„Webçˆ¬è™«å®ä¾‹
    pub fn new(config: WebCrawlerConfig) -> Self {
        Self { config }
    }

    /// ä½¿ç”¨é»˜è®¤é…ç½®åˆ›å»ºWebçˆ¬è™«
    pub fn with_url(url: &str) -> Self {
        let mut config = WebCrawlerConfig::default();
        config.url = url.to_string();
        Self::new(config)
    }

    /// è®¾ç½®è¾“å‡ºè·¯å¾„
    pub fn output_to<P: AsRef<Path>>(mut self, path: P) -> Self {
        self.config.output_path = path.as_ref().to_path_buf();
        self
    }

    /// é…ç½®èµ„æºåŒ…å«é€‰é¡¹
    pub fn include_resources(mut self, css: bool, js: bool, images: bool) -> Self {
        self.config.include_css = css;
        self.config.include_js = js;
        self.config.include_images = images;
        self
    }

    /// è®¾ç½®ç”¨æˆ·ä»£ç†
    pub fn user_agent(mut self, user_agent: &str) -> Self {
        self.config.user_agent = user_agent.to_string();
        self
    }

    /// è®¾ç½®è¿æ¥è¶…æ—¶
    pub fn timeout(mut self, seconds: u64) -> Self {
        self.config.timeout = seconds;
        self
    }

    /// æ‰§è¡Œç½‘é¡µçˆ¬å–
    /// 
    /// è¿”å›çˆ¬å–çš„HTMLå†…å®¹å­—ç¬¦ä¸²å’Œè¾“å‡ºæ–‡ä»¶çš„è·¯å¾„
    pub async fn crawl(&self) -> Result<(String, PathBuf)> {
        info!("ğŸ•·ï¸ å¼€å§‹çˆ¬å–ç½‘é¡µ: {}", self.config.url);
        debug!("çˆ¬è™«é…ç½®: {:?}", self.config);

        // éªŒè¯URL
        self.validate_url()?;

        // å‡†å¤‡è¾“å‡ºè·¯å¾„
        let output_path = self.prepare_output_path()?;
        debug!("è¾“å‡ºè·¯å¾„: {}", output_path.display());

        // ä½¿ç”¨é‡è¯•æœºåˆ¶çˆ¬å–ç½‘é¡µ
        let html_content = self.crawl_website_with_retry().await
            .with_context(|| format!("çˆ¬å–ç½‘é¡µå¤±è´¥: {}", self.config.url))?;

        // å†™å…¥åˆ°è¾“å‡ºæ–‡ä»¶ï¼ˆå¦‚æœæŒ‡å®šäº†è¾“å‡ºè·¯å¾„ï¼‰
        if self.config.output_path != PathBuf::new() {
            std::fs::write(&output_path, &html_content)
                .with_context(|| format!("å†™å…¥è¾“å‡ºæ–‡ä»¶å¤±è´¥: {}", output_path.display()))?;
            info!("âœ… ç½‘é¡µå·²ä¿å­˜åˆ°: {}", output_path.display());
        }

        Ok((html_content, output_path))
    }

    /// éªŒè¯URLæ ¼å¼
    fn validate_url(&self) -> Result<()> {
        if self.config.url.is_empty() {
            anyhow::bail!("URLä¸èƒ½ä¸ºç©º");
        }

        if !self.config.url.starts_with("http://") && !self.config.url.starts_with("https://") {
            anyhow::bail!("URLå¿…é¡»ä»¥http://æˆ–https://å¼€å¤´");
        }

        Ok(())
    }

    /// å‡†å¤‡è¾“å‡ºè·¯å¾„
    fn prepare_output_path(&self) -> Result<PathBuf> {
        let output_path = if self.config.output_path == PathBuf::new() {
            // å¦‚æœæ²¡æœ‰æŒ‡å®šè¾“å‡ºè·¯å¾„ï¼Œæ ¹æ®URLç”Ÿæˆé»˜è®¤è·¯å¾„
            self.generate_default_output_path()?
        } else {
            self.config.output_path.clone()
        };

        // ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        if let Some(parent) = output_path.parent() {
            std::fs::create_dir_all(parent)
                .with_context(|| format!("åˆ›å»ºè¾“å‡ºç›®å½•å¤±è´¥: {}", parent.display()))?;
        }

        Ok(output_path)
    }

    /// æ ¹æ®URLç”Ÿæˆé»˜è®¤è¾“å‡ºæ–‡ä»¶å
    fn generate_default_output_path(&self) -> Result<PathBuf> {
        use url::Url;
        
        let parsed_url = Url::parse(&self.config.url)
            .with_context(|| format!("è§£æURLå¤±è´¥: {}", self.config.url))?;
        
        let host = parsed_url.host_str().unwrap_or("unknown");
        let path = parsed_url.path();
        
        // ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å
        let mut filename = if path == "/" || path.is_empty() {
            format!("{}_index", host)
        } else {
            format!("{}{}", host, path.replace('/', "_"))
        };
        
        // æ¸…ç†æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦
        filename = filename
            .replace(['<', '>', ':', '"', '|', '?', '*'], "_")
            .replace("__", "_")
            .trim_matches('_')
            .to_string();
        
        // æ·»åŠ æ—¶é—´æˆ³é¿å…å†²çª
        let timestamp = std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .unwrap()
            .as_secs();
        
        let output_filename = format!("{}_{}.html", filename, timestamp);
        Ok(std::env::current_dir()?.join(output_filename))
    }
}

/// ä¾¿æ·å‡½æ•°ï¼šå¿«é€Ÿçˆ¬å–ç½‘é¡µåˆ°æŒ‡å®šè·¯å¾„
pub async fn crawl_url_to_file<P: AsRef<Path>>(
    url: &str,
    output_path: P,
) -> Result<String> {
    let crawler = WebCrawler::with_url(url).output_to(output_path);
    let (content, _) = crawler.crawl().await?;
    Ok(content)
}

/// ä¾¿æ·å‡½æ•°ï¼šçˆ¬å–ç½‘é¡µå¹¶è¿”å›HTMLå†…å®¹ï¼ˆä¸ä¿å­˜åˆ°æ–‡ä»¶ï¼‰
pub async fn crawl_url_to_string(url: &str) -> Result<String> {
    let temp_path = std::env::temp_dir().join("temp_crawl.html");
    crawl_url_to_file(url, &temp_path).await
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_web_crawler_config_default() {
        let config = WebCrawlerConfig::default();
        assert!(config.url.is_empty());
        assert_eq!(config.include_css, true);
        assert_eq!(config.include_js, false);
        assert_eq!(config.include_images, true);
        assert_eq!(config.timeout, 30);
        assert_eq!(config.user_agent, "translation-cli/0.1.0 (Monolith Web Crawler)");
    }

    #[test]
    fn test_web_crawler_builder() {
        let crawler = WebCrawler::with_url("https://example.com")
            .output_to("output.html")
            .include_resources(true, true, false)
            .user_agent("test-agent")
            .timeout(60);

        assert_eq!(crawler.config.url, "https://example.com");
        assert_eq!(crawler.config.output_path, PathBuf::from("output.html"));
        assert_eq!(crawler.config.include_css, true);
        assert_eq!(crawler.config.include_js, true);
        assert_eq!(crawler.config.include_images, false);
        assert_eq!(crawler.config.user_agent, "test-agent");
        assert_eq!(crawler.config.timeout, 60);
    }

    #[test]
    fn test_url_validation() {
        let crawler = WebCrawler::with_url("");
        assert!(crawler.validate_url().is_err());

        let crawler = WebCrawler::with_url("ftp://example.com");
        assert!(crawler.validate_url().is_err());

        let crawler = WebCrawler::with_url("https://example.com");
        assert!(crawler.validate_url().is_ok());

        let crawler = WebCrawler::with_url("http://example.com");
        assert!(crawler.validate_url().is_ok());
    }

    #[test]
    fn test_generate_default_output_path() {
        let crawler = WebCrawler::with_url("https://example.com/path/to/page");
        let path = crawler.generate_default_output_path().unwrap();
        
        let filename = path.file_name().unwrap().to_string_lossy();
        assert!(filename.contains("example.com"));
        assert!(filename.contains("path_to_page"));
        assert!(filename.ends_with(".html"));
    }

    #[test]
    fn test_generate_default_output_path_root() {
        let crawler = WebCrawler::with_url("https://example.com/");
        let path = crawler.generate_default_output_path().unwrap();
        
        let filename = path.file_name().unwrap().to_string_lossy();
        assert!(filename.contains("example.com_index"));
        assert!(filename.ends_with(".html"));
    }

    #[test]
    fn test_filename_sanitization() {
        let crawler = WebCrawler::with_url("https://example.com/path:with<bad>chars?query=1");
        let path = crawler.generate_default_output_path().unwrap();
        
        let filename = path.file_name().unwrap().to_string_lossy();
        // ç¡®ä¿æ²¡æœ‰éæ³•å­—ç¬¦
        assert!(!filename.contains('<'));
        assert!(!filename.contains('>'));
        assert!(!filename.contains(':'));
        assert!(!filename.contains('?'));
    }

    #[tokio::test]
    async fn test_crawl_invalid_url() {
        let crawler = WebCrawler::with_url("invalid-url");
        let result = crawler.crawl().await;
        assert!(result.is_err());
    }

    #[tokio::test]
    async fn test_crawl_nonexistent_domain() {
        let crawler = WebCrawler::with_url("https://this-domain-should-not-exist-12345.com")
            .timeout(5); // çŸ­è¶…æ—¶é¿å…æµ‹è¯•æ—¶é—´è¿‡é•¿
        
        let result = crawler.crawl().await;
        // åº”è¯¥å¤±è´¥ï¼Œä½†æˆ‘ä»¬ä¸æ£€æŸ¥å…·ä½“é”™è¯¯ç±»å‹ï¼Œå› ä¸ºå¯èƒ½å› ç½‘ç»œç¯å¢ƒè€Œå¼‚
        assert!(result.is_err());
    }

    // æ¨¡æ‹Ÿæµ‹è¯• - æµ‹è¯•çˆ¬è™«é…ç½®å’ŒåŸºæœ¬åŠŸèƒ½
    #[test]
    fn test_crawl_workflow_components() {
        // æµ‹è¯•URLéªŒè¯
        let valid_urls = vec![
            "https://example.com",
            "http://test.org",
            "https://subdomain.example.com/path",
        ];
        
        for url in valid_urls {
            let crawler = WebCrawler::with_url(url);
            assert!(crawler.validate_url().is_ok(), "URL should be valid: {}", url);
        }

        // æµ‹è¯•æ— æ•ˆURL
        let invalid_urls = vec![
            "",
            "ftp://example.com",
            "example.com",
        ];
        
        for url in invalid_urls {
            let crawler = WebCrawler::with_url(url);
            assert!(crawler.validate_url().is_err(), "URL should be invalid: {}", url);
        }
    }

    #[test]
    fn test_output_path_preparation() {
        use std::env;
        
        // æµ‹è¯•é»˜è®¤è¾“å‡ºè·¯å¾„ç”Ÿæˆ
        let crawler = WebCrawler::with_url("https://example.com");
        let path = crawler.prepare_output_path().unwrap();
        assert!(path.is_absolute());
        assert!(path.to_string_lossy().contains("example.com"));
        
        // æµ‹è¯•æŒ‡å®šè¾“å‡ºè·¯å¾„
        let temp_dir = env::temp_dir();
        let output_path = temp_dir.join("test_output.html");
        let crawler = WebCrawler::with_url("https://example.com")
            .output_to(&output_path);
        
        let prepared_path = crawler.prepare_output_path().unwrap();
        assert_eq!(prepared_path, output_path);
    }
}